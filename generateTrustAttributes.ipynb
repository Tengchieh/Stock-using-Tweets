{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "import pprint\n",
    "import csv\n",
    "import re\n",
    "from datetime import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "company_list = []\n",
    "id_list = []\n",
    "with open('SP500.csv') as SP500csv:\n",
    "    reader = csv.DictReader(SP500csv)\n",
    "    for row in reader:\n",
    "        company_list.append(row['Symbol'])\n",
    "\n",
    "buy_keywords = []\n",
    "sell_keywords = []\n",
    "with open('keyword.csv') as keyword:\n",
    "    reader = csv.DictReader(keyword)\n",
    "    for row in reader:\n",
    "        buy_keywords.append(row['Buy'])\n",
    "        if row['Sell']:\n",
    "            sell_keywords.append(row['Sell'])\n",
    "\n",
    "def generateDiGraph(node_ids, edge_data):\n",
    "    # Generate network graph\n",
    "    MG = nx.MultiDiGraph()\n",
    "\n",
    "    for n in node_ids:\n",
    "        MG.add_node(n)\n",
    "\n",
    "    for d in edge_data:\n",
    "        MG.add_edge(d['source'], d['dest'], key=d['post_id'])\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for (u, v) in MG.edges():\n",
    "        G.add_edge(u, v, weight=len(MG[u][v]))\n",
    "    return G            \n",
    "\n",
    "def calculate_authority(G):\n",
    "    hubs, authorities = nx.hits(G)\n",
    "    return authorities\n",
    "\n",
    "attribute_list = ['n_tweet', 'n_stock_tweet', 'n_sentiment_tweet', 'statuses_count', 'followers_count', 'friends_count',\n",
    "                 'authority_score', 'reputation_score']\n",
    "                 #,'avg_quote', 'avg_stock_quote', 'avg_sentiment_quote', 'avg_reply', 'avg_stock_reply', 'avg_sentiment_reply',\n",
    "                 #'avg_retweet', 'avg_stock_retweet', 'avg_sentiment_retweet','avg_favorite', 'avg_stock_favorite', 'avg_sentiment_favorite']\n",
    "#attributes = len(attribute_list)\n",
    "\n",
    "def generateUserAttributes(year, month, source_dir, attribute_list):\n",
    "    attributes = len(attribute_list)\n",
    "    \n",
    "    with open(os.path.join(\"Extra_Storage\", source_dir, year, month, \"id_list.txt\" )) as id_list:\n",
    "        reader = id_list.readlines()\n",
    "        idList = [x.strip() for x in reader]\n",
    "        n_authors = len(idList)\n",
    "        user_attributes = [0] * n_authors\n",
    "        for i in range(n_authors):\n",
    "            user_attributes[i] = [0] * attributes\n",
    "\n",
    "    path = os.path.join(\"Extra_Storage\", source_dir, year, month)\n",
    "    node_ids = []\n",
    "    edge_data = []\n",
    "\n",
    "    for dirPath, dirNames, fileNames in os.walk(path):\n",
    "            for file in fileNames:\n",
    "                if file.endswith(\".json\"):    \n",
    "                        print(file)\n",
    "                        filepath = os.path.join(dirPath, file)\n",
    "                        with open(filepath) as json_data:\n",
    "                            data=[]\n",
    "                            for line in json_data:\n",
    "                                try:\n",
    "                                    data.append(json.loads(line))\n",
    "                                except json.JSONDecodeError:\n",
    "                                    continue\n",
    "                            for tweet in data:\n",
    "                                if 'text' in tweet:\n",
    "                                    if 'user' in tweet:\n",
    "                                        idIndex = idList.index(str(tweet['user']['id']))                                    \n",
    "                                    else:\n",
    "                                        break\n",
    "\n",
    "                                    # Update statuses_count\n",
    "                                    if tweet['user']['statuses_count'] > user_attributes[idIndex][attribute_list.index('statuses_count')]:\n",
    "                                        user_attributes[idIndex][attribute_list.index('statuses_count')] = tweet['user']['statuses_count']\n",
    "                                    # Update followers_count\n",
    "                                    if tweet['user']['followers_count'] > user_attributes[idIndex][attribute_list.index('followers_count')]:\n",
    "                                        user_attributes[idIndex][attribute_list.index('followers_count')] = tweet['user']['followers_count']\n",
    "                                    # Update friends_count\n",
    "                                    if tweet['user']['friends_count'] > user_attributes[idIndex][attribute_list.index('friends_count')]:\n",
    "                                        user_attributes[idIndex][attribute_list.index('friends_count')] = tweet['user']['friends_count']\n",
    "\n",
    "    #                                # Update avg_quote\n",
    "    #                                if 'quote_count' in tweet:\n",
    "    #                                    user_attributes[idIndex][attribute_list.index('avg_quote')] += tweet['quote_count']\n",
    "    #                                # Update avg_reply\n",
    "    #                                if 'reply_count' in tweet:\n",
    "    #                                    avgreply = (user_attributes[idIndex][attribute_list.index('n_tweet')]*user_attributes[idIndex][attribute_list.index('avg_reply')]  \n",
    "    #                                    + tweet['reply_count'])/(user_attributes[idIndex][attribute_list.index('n_tweet')]+1)\n",
    "    #                                    user_attributes[idIndex][attribute_list.index('avg_reply')] = avgreply\n",
    "    #                                # Update avg_retweet\n",
    "    #                                if 'retweet_count' in tweet:\n",
    "    #                                    avgretweet = (user_attributes[idIndex][attribute_list.index('n_tweet')]*user_attributes[idIndex][attribute_list.index('avg_retweet')]  \n",
    "    #                                    + tweet['retweet_count'])/(user_attributes[idIndex][attribute_list.index('n_tweet')]+1)\n",
    "    #                                    user_attributes[idIndex][attribute_list.index('avg_retweet')] = avgretweet\n",
    "    #                                    #user_attributes[idIndex][attribute_list.index('avg_retweet')] += tweet['retweet_count']\n",
    "    #                                # Update avg_favorite  \n",
    "    #                                if 'favorite_count' in tweet:\n",
    "    #                                    user_attributes[idIndex][attribute_list.index('avg_favorite')] += tweet['favorite_count']\n",
    "\n",
    "                                    if 'quoted_status' in tweet:\n",
    "                                        if not tweet['user']['id'] in node_ids:\n",
    "                                            node_ids.append(tweet['user']['id'])\n",
    "                                        if not tweet['quoted_status']['user']['id'] in node_ids:\n",
    "                                            node_ids.append(tweet['quoted_status']['user']['id'])\n",
    "                                        edge_data.append({'source':tweet['user']['id'], 'dest': tweet['quoted_status']['user']['id'], 'post_id': tweet['id']})\n",
    "                                    if 'retweeted_status' in tweet:\n",
    "                                        if not tweet['user']['id'] in node_ids:\n",
    "                                            node_ids.append(tweet['user']['id'])\n",
    "                                        if not tweet['retweeted_status']['user']['id'] in node_ids:\n",
    "                                            node_ids.append(tweet['retweeted_status']['user']['id'])\n",
    "                                        edge_data.append({'source':tweet['user']['id'], 'dest': tweet['retweeted_status']['user']['id'], 'post_id': tweet['id']})\n",
    "\n",
    "\n",
    "                                    user_attributes[idIndex][attribute_list.index('n_tweet')]+=1     #Number of all tweets plus 1\n",
    "                                    tweet_lower = tweet['text'].lower()\n",
    "                                    company_index = -1\n",
    "                                    for company in company_list:\n",
    "                                        company_index+=1\n",
    "                                        company_tag = ' $' + company + ' '\n",
    "                                        if company_tag.lower() in tweet_lower:\n",
    "                                            # Update avg_stock_quote\n",
    "    #                                        if 'quote_count' in tweet:\n",
    "    #                                            avgquote = (user_attributes[idIndex][attribute_list.index('n_stock_tweet')]*user_attributes[idIndex][attribute_list.index('avg_stock_quote')]  \\\n",
    "    #                                            + tweet['quote_count'])/(user_attributes[idIndex][attribute_list.index('n_stock_tweet')]+1)\n",
    "    #                                            user_attributes[idIndex][attribute_list.index('avg_stock_quote')] = avgquote                                \n",
    "    #                                        # Update avg_stock_reply\n",
    "    #                                        if 'reply_count' in tweet:\n",
    "    #                                            avgreply = (user_attributes[idIndex][attribute_list.index('n_stock_tweet')]*user_attributes[idIndex][attribute_list.index('avg_stock_reply')]  \\\n",
    "    #                                            + tweet['reply_count'])/(user_attributes[idIndex][attribute_list.index('n_stock_tweet')]+1)\n",
    "    #                                            user_attributes[idIndex][attribute_list.index('avg_stock_reply')] = avgreply\n",
    "    #                                        # Update avg_stock_retweet\n",
    "    #                                        if 'retweet_count' in tweet:\n",
    "    #                                            avgretweet = (user_attributes[idIndex][attribute_list.index('n_tweet')]*user_attributes[idIndex][attribute_list.index('avg_stock_retweet')]  \n",
    "    #                                            + tweet['retweet_count'])/(user_attributes[idIndex][attribute_list.index('n_stock_tweet')]+1)\n",
    "    #                                            user_attributes[idIndex][attribute_list.index('avg_stock_retweet')] = avgretweet\n",
    "    #                                        # Update avg_stock_favorite\n",
    "    #                                        if 'favorite_count' in tweet:\n",
    "    #                                            avgfavorite = (user_attributes[idIndex][attribute_list.index('n_stock_tweet')]*user_attributes[idIndex][attribute_list.index('avg_stock_favorite')]  \n",
    "    #                                            + tweet['favorite_count'])/(user_attributes[idIndex][attribute_list.index('n_stock_tweet')]+1)\n",
    "    #                                            user_attributes[idIndex][attribute_list.index('avg_stock_favorite')] = avgfavorite\n",
    "\n",
    "                                            user_attributes[idIndex][attribute_list.index('n_stock_tweet')]+=1     #Number of stock-related tweets plus 1\n",
    "                                            sentiment = False\n",
    "                                            for buy_keyword in buy_keywords:\n",
    "                                                if buy_keyword in tweet_lower:\n",
    "                                                    sentiment = True\n",
    "                                                    break\n",
    "                                            for sell_keyword in sell_keywords:\n",
    "                                                if sell_keyword in tweet_lower:\n",
    "                                                    sentiment = True\n",
    "                                                    break\n",
    "                                            if sentiment:\n",
    "\n",
    "    #                                            # Update avg_sentiment_quote\n",
    "    #                                            if 'quote_count' in tweet:\n",
    "    #                                                avgquote = (user_attributes[idIndex][attribute_list.index('n_sentiment_tweet')]*user_attributes[idIndex][attribute_list.index('avg_sentiment_quote')]  \n",
    "    #                                                + tweet['quote_count'])/(user_attributes[idIndex][attribute_list.index('n_sentiment_tweet')]+1)\n",
    "    #                                                user_attributes[idIndex][attribute_list.index('avg_sentiment_quote')] = avgquote                                \n",
    "    #                                            # Update avg_sentiment_reply     \n",
    "    #                                            if 'reply_count' in tweet:\n",
    "    #                                                avgreply = (user_attributes[idIndex][attribute_list.index('n_sentiment_tweet')]*user_attributes[idIndex][attribute_list.index('avg_sentiment_reply')]  \n",
    "    #                                                + tweet['reply_count'])/(user_attributes[idIndex][attribute_list.index('n_sentiment_tweet')]+1)\n",
    "    #                                                user_attributes[idIndex][attribute_list.index('avg_sentiment_reply')] = avgreply\n",
    "    #                                            # Update avg_sentiment_retweet\n",
    "    #                                            if 'retweet_count' in tweet:\n",
    "    #                                                avgretweet = (user_attributes[idIndex][attribute_list.index('n_sentiment_tweet')]*user_attributes[idIndex][attribute_list.index('avg_sentiment_retweet')]  \n",
    "    #                                                + tweet['retweet_count'])/(user_attributes[idIndex][attribute_list.index('n_sentiment_tweet')]+1)\n",
    "    #                                                user_attributes[idIndex][attribute_list.index('avg_sentiment_retweet')] = avgretweet\n",
    "    #                                            # Update avg_sentiment_favorite \n",
    "    #                                            if 'favorite_count' in tweet:\n",
    "    #                                                avgfavorite = (user_attributes[idIndex][attribute_list.index('n_sentiment_tweet')]*user_attributes[idIndex][attribute_list.index('avg_sentiment_favorite')]  \n",
    "    #                                                + tweet['favorite_count'])/(user_attributes[idIndex][attribute_list.index('n_sentiment_tweet')]+1)\n",
    "    #                                                user_attributes[idIndex][attribute_list.index('avg_sentiment_favorite')] = avgfavorite\n",
    "\n",
    "                                                user_attributes[idIndex][attribute_list.index('n_sentiment_tweet')]+=1    #Number of sentiment-related tweets plus 1\n",
    "                                            break\n",
    "\n",
    "    G = generateDiGraph(node_ids, edge_data)\n",
    "\n",
    "    hubs, authority_scores = nx.hits(G, max_iter=2000, tol=1.0e-6 )\n",
    "    for node, value in authority_scores.items():\n",
    "        if str(node) in idList:\n",
    "            idIndex = idList.index(str(node))\n",
    "            user_attributes[idIndex][attribute_list.index('authority_score')] = value\n",
    "\n",
    "    reputation_scores = nx.pagerank(G)\n",
    "    for node, value in reputation_scores.items():\n",
    "        if str(node) in idList:\n",
    "            idIndex = idList.index(str(node))\n",
    "            user_attributes[idIndex][attribute_list.index('reputation_score')] = value\n",
    "\n",
    "    arr = np.array(user_attributes)\n",
    "    savepath = os.path.join(\"Extra_Storage\", source_dir, year, month,'user_attributes_' + year + '_' + month + '.csv' )\n",
    "    #np.savetxt(savepath, arr, fmt='%1.5f', delimiter=',')\n",
    "    attributes = ','.join(attribute_list)\n",
    "    np.savetxt(savepath, arr, header=attributes, delimiter=',')\n",
    "\n",
    "#generateUserAttributes('2016', '03', 'Archiveteam_Filtered', attribute_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess and generate weighting for sentiments by users\n",
    "from sklearn import linear_model\n",
    "\n",
    "def preprocessing(year, month, source_dir):\n",
    "    path = os.path.join(\"Extra_Storage\", source_dir, year, month)\n",
    "\n",
    "    preprocessedATT = []\n",
    "    with open(os.path.join(path, \"user_attributes_\"+ year + \"_\" + month + \".csv\" )) as tweet_attributes:\n",
    "        reader = csv.DictReader(tweet_attributes)\n",
    "        for row in reader:\n",
    "            tempATT = []\n",
    "            if(float(row['# n_tweet'])!=0):\n",
    "                tempATT.append(float(row['n_stock_tweet'])/float(row['# n_tweet'])) #expertise\n",
    "            else:\n",
    "                tempATT.append(0)\n",
    "            #tempATT.append(float(row['n_stock_tweet']))\n",
    "            #tempATT.append(float(row['n_sentiment_tweet']))\n",
    "            #tempATT.append(float(row['statuses_count']))\n",
    "            #tempATT.append(float(row['followers_count']))\n",
    "            #tempATT.append(float(row['friends_count']))\n",
    "            if(float(row['authority_score'])==0):\n",
    "                tempATT.append(0)\n",
    "            else:\n",
    "                #tempATT.append(math.log10(float(row['authority_score']))+100)\n",
    "                tempATT.append(float(row['authority_score']))\n",
    "            if(float(row['reputation_score'])==0):\n",
    "                tempATT.append(0)\n",
    "            else:\n",
    "                #tempATT.append(math.log10(float(row['reputation_score']))+50)\n",
    "                tempATT.append(float(row['reputation_score']))\n",
    "            preprocessedATT.append(tempATT)\n",
    "        \n",
    "        avg_expertise = np.array(preprocessedATT).mean(axis=0)[0]\n",
    "        for item in preprocessedATT:\n",
    "            item.append(1-abs(avg_expertise - item[0])) #experience\n",
    "        #temp = np.array(preprocessedATT).min(axis=0)\n",
    "        #for item in preprocessedATT:\n",
    "        #    item[6] = item[6] + temp[6]\n",
    "        #    item[7] = item[7] + temp[7]\n",
    "        \n",
    "    arr = np.array(preprocessedATT)\n",
    "    savepath = os.path.join(\"Extra_Storage\", source_dir, year, month,'trust_scores_'+ year + '_' + month + '.csv' )\n",
    "    #filter_names = 'expertise_score,n_stock_tweet,n_sentiment_tweet,statuses_count,followers_count,friends_count,authority_score,reputation_score,experience_score'\n",
    "    filter_names = 'expertise_score,authority_score,reputation_score,experience_score'\n",
    "    np.savetxt(savepath, arr, header=filter_names, delimiter=',', comments='')\n",
    "    return arr\n",
    "\n",
    "#ATT = preprocessing('2016', '03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01.json\n",
      "03.json\n",
      "08.json\n",
      "10.json\n",
      "11.json\n",
      "12.json\n",
      "13.json\n",
      "14.json\n",
      "16.json\n",
      "17.json\n",
      "18.json\n",
      "19.json\n",
      "20.json\n",
      "21.json\n",
      "22.json\n",
      "24.json\n",
      "25.json\n",
      "26.json\n",
      "27.json\n",
      "28.json\n",
      "29.json\n",
      "30.json\n",
      "31.json\n",
      "01.json\n",
      "02.json\n",
      "03.json\n",
      "04.json\n",
      "05.json\n",
      "06.json\n",
      "07.json\n",
      "08.json\n",
      "09.json\n",
      "14.json\n",
      "15.json\n",
      "16.json\n",
      "17.json\n",
      "18.json\n",
      "19.json\n",
      "20.json\n",
      "21.json\n",
      "22.json\n",
      "23.json\n",
      "24.json\n",
      "25.json\n",
      "26.json\n",
      "01.json\n",
      "02.json\n",
      "03.json\n",
      "04.json\n",
      "05.json\n",
      "06.json\n",
      "07.json\n",
      "08.json\n",
      "09.json\n",
      "10.json\n",
      "11.json\n",
      "12.json\n",
      "13.json\n",
      "14.json\n",
      "15.json\n",
      "16.json\n",
      "17.json\n",
      "18.json\n",
      "19.json\n",
      "20.json\n",
      "21.json\n",
      "22.json\n",
      "23.json\n",
      "24.json\n",
      "25.json\n",
      "26.json\n",
      "27.json\n",
      "28.json\n",
      "29.json\n",
      "30.json\n",
      "31.json\n",
      "01.json\n",
      "02.json\n",
      "03.json\n",
      "04.json\n",
      "05.json\n",
      "15.json\n",
      "16.json\n",
      "17.json\n",
      "18.json\n",
      "19.json\n",
      "20.json\n",
      "21.json\n",
      "22.json\n",
      "23.json\n",
      "24.json\n",
      "25.json\n",
      "26.json\n",
      "27.json\n",
      "28.json\n",
      "29.json\n",
      "31.json\n",
      "02.json\n",
      "03.json\n",
      "04.json\n",
      "05.json\n",
      "06.json\n",
      "07.json\n",
      "08.json\n",
      "09.json\n",
      "10.json\n",
      "11.json\n",
      "12.json\n",
      "13.json\n",
      "14.json\n",
      "15.json\n",
      "17.json\n",
      "18.json\n",
      "19.json\n",
      "20.json\n",
      "21.json\n",
      "22.json\n",
      "30.json\n"
     ]
    }
   ],
   "source": [
    "#generateUserAttributes('2016', '05', 'ArchiveteamTest', attribute_list)\n",
    "\n",
    "#generateUserAttributes('2016', '04', 'ArchiveteamTest', attribute_list)\n",
    "#ATT = preprocessing('2016', '04', 'ArchiveteamTest')\n",
    "generateUserAttributes('2016', '03', 'ArchiveteamTest', attribute_list)\n",
    "ATT = preprocessing('2016', '03', 'ArchiveteamTest')\n",
    "generateUserAttributes('2016', '02', 'ArchiveteamTest', attribute_list)\n",
    "ATT = preprocessing('2016', '02', 'ArchiveteamTest')\n",
    "generateUserAttributes('2016', '01', 'ArchiveteamTest', attribute_list)\n",
    "ATT = preprocessing('2016', '01', 'ArchiveteamTest')\n",
    "generateUserAttributes('2015', '12', 'ArchiveteamTest', attribute_list)\n",
    "ATT = preprocessing('2015', '12', 'ArchiveteamTest')\n",
    "generateUserAttributes('2015', '11', 'ArchiveteamTest', attribute_list)\n",
    "ATT = preprocessing('2015', '11', 'ArchiveteamTest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6.45543152e-01  -1.00942015e-01   3.05676053e+02  -2.48605762e-01]\n"
     ]
    }
   ],
   "source": [
    "def combineFilters(source_dir, source_year_start, source_month_start, source_year_end, source_month_end, target_year, target_month):\n",
    "    y=[]\n",
    "    X=[[], [], [], [] ]\n",
    "    \n",
    "    for year_int in range(int(source_year_start), int(source_year_end)+1):\n",
    "        if (year_int == int(source_year_end)):\n",
    "            last_month = int(source_month_end)            \n",
    "        else:\n",
    "            last_month = 12\n",
    "        if(year_int == int(source_year_start)):\n",
    "            first_month = int(source_month_start)\n",
    "        else:\n",
    "            first_month = 1\n",
    "        cur_year = str(year_int)\n",
    "        \n",
    "        for month_int in range(first_month, last_month+1):\n",
    "            if(month_int<10):\n",
    "                cur_month = '0'+ str(month_int)\n",
    "            else:\n",
    "                cur_month = str(month_int)\n",
    "                \n",
    "            path = os.path.join(\"Extra_Storage\", source_dir, cur_year, cur_month)\n",
    "            ROI_list = []\n",
    "            with open(os.path.join(path, \"Acc_ROI_all_\"+ cur_year + \"_\" + cur_month + \".csv\")) as ROIcsv:\n",
    "                reader = csv.reader(ROIcsv)\n",
    "                for row in reader:\n",
    "                    ROI_list.append(float(row[0]))\n",
    "\n",
    "            y.extend(ROI_list)\n",
    "\n",
    "            ATT = preprocessing(cur_year, cur_month, source_dir)\n",
    "            X[0].extend(list(ATT[:,0]))\n",
    "            X[1].extend(list(ATT[:,1]))\n",
    "            X[2].extend(list(ATT[:,2]))\n",
    "            X[3].extend(list(ATT[:,3]))\n",
    "            \n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(np.array(X).transpose(), y)\n",
    "    print(regr.coef_)\n",
    "    \n",
    "    ATT = preprocessing(target_year, target_month, source_dir)\n",
    "    X_new = [list(ATT[:,0]),list(ATT[:,1]),list(ATT[:,2]),list(ATT[:,3])]\n",
    "    Y_new = regr.predict(np.array(X_new).transpose())\n",
    "    \n",
    "    savepath = os.path.join(\"Extra_Storage\", source_dir, target_year, target_month,'predicted_scores_'+ target_year + '_' + target_month + '_from_' + source_year_start + '_' + source_month_start + '_to_' + source_year_end + '_' + source_month_end + '.csv' )\n",
    "    np.savetxt(savepath, Y_new, delimiter=',', comments='', header = 'combined_score')\n",
    "\n",
    "#combineFilters(\"Archiveteam_Halfyear\", \"2016\", \"03\", \"2016\", \"05\")\n",
    "combineFilters(\"Archiveteam_Halfyear\", \"2015\", \"11\", \"2016\", \"04\", \"2016\", \"05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "import pprint\n",
    "import csv\n",
    "import re\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "company_list = []\n",
    "id_list = []\n",
    "with open('SP500.csv') as SP500csv:\n",
    "    reader = csv.DictReader(SP500csv)\n",
    "    for row in reader:\n",
    "        company_list.append(row['Symbol'])\n",
    "\n",
    "buy_keywords = []\n",
    "sell_keywords = []\n",
    "with open('keyword.csv') as keyword:\n",
    "    reader = csv.DictReader(keyword)\n",
    "    for row in reader:\n",
    "        buy_keywords.append(row['Buy'])\n",
    "        if row['Sell']:\n",
    "            sell_keywords.append(row['Sell'])\n",
    "\n",
    "attributes = 3\n",
    "with open(os.path.join(\"Extra_Storage\", \"Archiveteam_Filtered\", \"2016\", \"04\", \"id_list.txt\" )) as id_list:\n",
    "    reader = id_list.readlines()\n",
    "    idList = [x.strip() for x in reader]\n",
    "    n_authors = len(idList)\n",
    "    user_attributes = [0] * n_authors\n",
    "    for i in range(n_authors):\n",
    "        user_attributes[i] = [0] * attributes\n",
    "\n",
    "path = os.path.join(\"Extra_Storage\", \"Archiveteam_Filtered\", \"2016\", \"04\")\n",
    "#for run_date in range(1,30):\n",
    "#    if run_date < 10:\n",
    "#        sub_path = os.path.join(path, \"0\" + str(run_date))\n",
    "#    else:\n",
    "#        sub_path = os.path.join(path, str(run_date))\n",
    "        \n",
    "for dirPath, dirNames, fileNames in os.walk(path):\n",
    "        for file in fileNames:\n",
    "            if file.endswith(\".json\"):    \n",
    "                    filepath = os.path.join(dirPath, file)\n",
    "                    with open(filepath) as json_data:\n",
    "                        data=[]\n",
    "                        for line in json_data:\n",
    "                            try:\n",
    "                                data.append(json.loads(line))\n",
    "                            except json.JSONDecodeError:\n",
    "                                continue\n",
    "                        for tweet in data:\n",
    "                            if 'text' in tweet:\n",
    "                                if 'user' in tweet:\n",
    "                                    idIndex = idList.index(str(tweet['user']['id']))\n",
    "                                    #print(idIndex)\n",
    "                                else:\n",
    "                                    break\n",
    "                                user_attributes[idIndex][0]+=1     #Number of all tweets plus 1\n",
    "                                tweet_lower = tweet['text'].lower()\n",
    "                                company_index = -1\n",
    "                                for company in company_list:\n",
    "                                    company_index+=1\n",
    "                                    company_tag = ' $' + company + ' '\n",
    "                                    if company_tag.lower() in tweet_lower:\n",
    "                                        user_attributes[idIndex][1]+=1     #Number of stock-related tweets plus 1\n",
    "                                        sentiment = False\n",
    "                                        for buy_keyword in buy_keywords:\n",
    "                                            if buy_keyword in tweet_lower:\n",
    "                                                sentiment = True\n",
    "                                                break\n",
    "                                        for sell_keyword in sell_keywords:\n",
    "                                            if sell_keyword in tweet_lower:\n",
    "                                                sentiment = True\n",
    "                                                break\n",
    "                                        if sentiment:\n",
    "                                            user_attributes[idIndex][2]+=1    #Number of sentiment-related tweets plus 1\n",
    "                                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = np.array(user_attributes)\n",
    "savepath = os.path.join(\"Extra_Storage\", \"Archiveteam_Filtered\", \"2016\", \"03\",'user_attributes_2016_03.csv' )\n",
    "np.savetxt(savepath, arr, fmt='%1.3f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_author_list = []\n",
    "savepath = os.path.join(\"Extra_Storage\", \"Archiveteam_Filtered\", \"2016\", \"03\" )\n",
    "\n",
    "with open(os.path.join(\"Extra_Storage\", \"Archiveteam_Filtered\", \"2016\", \"03\", \"id_list.txt\" )) as id_list:\n",
    "    reader = id_list.readlines()\n",
    "    idList = [x.strip() for x in reader]\n",
    "    \n",
    "with open(os.path.join(savepath, 'user_attributes_2016_03.csv')) as tweetCount:\n",
    "    reader = csv.reader(tweetCount)\n",
    "    i=0\n",
    "    for row in reader:\n",
    "        if int(row[1]) >= 20:\n",
    "            #print(row[1])\n",
    "            top_author_list.append(idList[i])\n",
    "        i+=1\n",
    "\n",
    "file = open(os.path.join(savepath, \"Morethan20id_list.txt\"), 'w')\n",
    "for item in top_author_list:\n",
    "    file.write(\"%s\\n\" %item)\n",
    "file.close()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "\n",
    "with open(os.path.join(\"Extra_Storage\", \"Archiveteam_Filtered\", \"2016\", \"04\", \"Morethan20id_list.txt\" )) as id_list:\n",
    "    reader = id_list.readlines()\n",
    "    top_author_list = [x.strip() for x in reader]\n",
    "\n",
    "path = os.path.join(\"Extra_Storage\", \"Archiveteam_Filtered\", \"2016\", \"04\")\n",
    "textfile = open(os.path.join(path, \"Morethan20_text.txt\"), 'w')\n",
    "company_list = []\n",
    "with open('SP500.csv') as SP500csv:\n",
    "    reader = csv.DictReader(SP500csv)\n",
    "    for row in reader:\n",
    "        company_list.append(row['Symbol'])\n",
    "\n",
    "for dirPath, dirNames, fileNames in os.walk(path):\n",
    "    for file in fileNames:\n",
    "        if file.endswith(\"json\"):\n",
    "            filepath = os.path.join(dirPath, file)\n",
    "            with open(filepath) as json_data:\n",
    "                data=[]\n",
    "                for line in json_data:\n",
    "                    try:\n",
    "                        data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "                for tweet in data:\n",
    "                    if 'text' in tweet:\n",
    "                        if str(tweet['user']['id']) in top_author_list:\n",
    "                            for company in company_list:\n",
    "                                company_tag = ' $' + company + ' '\n",
    "                                if company_tag in tweet['text']:\n",
    "                                    #print(tweet['text'])\n",
    "                                    textfile.write(\"%s\\n\" %tweet['text'].encode(\"utf-8\"))\n",
    "                                    #textfile.write(tweet['text']+\"\\n\")\n",
    "                                    break\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
