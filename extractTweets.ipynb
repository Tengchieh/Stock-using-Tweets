{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "import pprint\n",
    "import csv\n",
    "import re\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "company_list = []\n",
    "all_tweet = ''\n",
    "id_list = []\n",
    "with open('SP500.csv') as SP500csv:\n",
    "    reader = csv.DictReader(SP500csv)\n",
    "    for row in reader:\n",
    "        company_list.append(row['Symbol'])\n",
    "        \n",
    "path = os.path.join(\"Extra_Storage\", \"Archiveteam\", \"archiveteam-twitter-stream-2016-05\", \"2016\", \"05\")\n",
    "for run_date in range(1,32):\n",
    "    if run_date < 10:\n",
    "        sub_path = os.path.join(path, \"0\" + str(run_date))\n",
    "    else:\n",
    "        sub_path = os.path.join(path, str(run_date))\n",
    "        \n",
    "    for dirPath, dirNames, fileNames in os.walk(sub_path):\n",
    "        for file in fileNames:\n",
    "            if file.endswith(\"json\"):    \n",
    "                    filepath = os.path.join(dirPath, file)\n",
    "                    with open(filepath) as json_data:\n",
    "                        data=[]\n",
    "                        all_tweet = ''\n",
    "                        for line in json_data:\n",
    "                            try:\n",
    "                                data.append(json.loads(line))\n",
    "                            except json.JSONDecodeError:\n",
    "                                continue                            \n",
    "                        for tweet in data:\n",
    "                            if 'text' in tweet:\n",
    "                                tweet_lower = tweet['text'].lower()\n",
    "                                for company in company_list:\n",
    "                                    company_tag = ' $' + company + ' '\n",
    "                                    if company_tag.lower() in tweet_lower:\n",
    "                                        if tweet['user']['id'] in id_list:\n",
    "                                            break\n",
    "                                        else:\n",
    "                                            id_list.append(tweet['user']['id'])                                      \n",
    "                                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path =  os.path.join(\"Extra_Storage\", \"Archiveteam_Filtered\", \"2015\", \"11\", 'id_list.txt')\n",
    "file = open(save_path, 'w')\n",
    "for item in id_list:\n",
    "    file.write(\"%s\\n\" %item)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine ID List of different months into one file.\n",
    "from dateutil.relativedelta import relativedelta\n",
    "def combineIDList(start_year, start_month, end_year, end_month):\n",
    "    combine_idList = []\n",
    "    start_date = datetime(start_year, start_month, 1)\n",
    "    end_date = datetime(end_year, end_month, 2)\n",
    "    cur_date = start_date\n",
    "    \n",
    "    while (end_date - cur_date).days > 0:\n",
    "        month = cur_date.month\n",
    "        year = cur_date.year\n",
    "        if month < 10:\n",
    "            month_str = '0' + str(month)\n",
    "        else:\n",
    "            month_str = str(month)\n",
    "            \n",
    "        search_path =  os.path.join(\"Extra_Storage\", \"Archiveteam_Filtered\", str(year), month_str, 'id_list.txt')\n",
    "        with open(search_path) as id_list:\n",
    "            reader = id_list.readlines()\n",
    "            idList = [x.strip() for x in reader]\n",
    "        combine_idList = combine_idList + list(set(idList) - set(combine_idList))\n",
    "        \n",
    "        cur_date = cur_date + relativedelta(months=1)\n",
    "    \n",
    "    save_path = os.path.join(\"Extra_Storage\", \"Archiveteam_Halfyear\")\n",
    "    for dirPath, dirNames, fileNames in os.walk(save_path):\n",
    "        if not dirNames:\n",
    "            file = open(os.path.join(dirPath, 'id_list.txt') , 'w')\n",
    "            for item in combine_idList:\n",
    "                file.write(\"%s\\n\" %item)\n",
    "            file.close()\n",
    "\n",
    "combineIDList(2015, 11, 2016, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\01.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\02.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\03.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\04.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\05.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\06.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\07.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\08.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\09.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\10.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\11.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\12.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\13.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\14.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\15.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\16.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\17.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\18.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\19.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\20.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\21.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\22.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\23.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\24.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\25.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\26.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\27.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\28.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\29.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\30.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\04\\31.json\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "import pprint\n",
    "import csv\n",
    "import re\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "def extractTweets(folder_name, year, month):\n",
    "    path = os.path.join(\"Extra_Storage\", \"Archiveteam\", \"archiveteam-twitter-stream-\"+ year + \"-\" + month, year, month)\n",
    "    save_path =  os.path.join(\"Extra_Storage\", folder_name, year, month)\n",
    "    \n",
    "    with open(os.path.join(save_path, \"id_list.txt\")) as id_list:\n",
    "            reader = id_list.readlines()\n",
    "            idList = [x.strip() for x in reader]\n",
    "    \n",
    "    for run_date in range(1,32):\n",
    "        if run_date < 10:\n",
    "            sub_path = os.path.join(path, \"0\" + str(run_date))\n",
    "            sub_save_path = os.path.join(save_path, \"0\" + str(run_date) + \".json\")\n",
    "        else:\n",
    "            sub_path = os.path.join(path, str(run_date))\n",
    "            sub_save_path = os.path.join(save_path, str(run_date) + \".json\")\n",
    "        print(sub_save_path)    \n",
    "        for dirPath, dirNames, fileNames in os.walk(sub_path):\n",
    "            for file in fileNames:\n",
    "                if file.endswith(\"json\"):    \n",
    "                        filepath = os.path.join(dirPath, file)\n",
    "                        with open(filepath, 'r', encoding='utf-8-sig') as json_data:                            \n",
    "                            data=[]                             \n",
    "                            for line in json_data:\n",
    "                                try:\n",
    "                                    data.append(json.loads(line))\n",
    "                                except json.JSONDecodeError:\n",
    "                                    continue\n",
    "\n",
    "                            #with open(sub_save_path, mode='w') as f:\n",
    "                            #    json.dump([], f)\n",
    "                            for tweet in data:\n",
    "                                if 'user' in tweet:\n",
    "                                    if str(tweet['user']['id']) in idList:\n",
    "                                        with open(sub_save_path, mode='a') as outfile:\n",
    "                                            json.dump(tweet, outfile)\n",
    "                                            outfile.write('\\n')\n",
    "                                            outfile.close\n",
    "\n",
    "\n",
    "#extractTweets('Archiveteam_Halfyear', '2016', '05')\n",
    "extractTweets('ArchiveteamTest', '2016', '04')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\01.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\02.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\03.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\04.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\05.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\06.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\07.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\08.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\09.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\10.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\11.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\12.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\13.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\14.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\15.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\16.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\17.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\18.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\19.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\20.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\21.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\22.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\23.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\24.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\25.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\26.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\27.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\28.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\29.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\30.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\03\\31.json\n"
     ]
    }
   ],
   "source": [
    "extractTweets('ArchiveteamTest', '2016', '03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\01.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\02.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\03.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\04.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\05.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\06.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\07.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\08.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\09.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\10.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\11.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\12.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\13.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\14.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\15.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\16.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\17.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\18.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\19.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\20.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\21.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\22.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\23.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\24.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\25.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\26.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\27.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\28.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\29.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\30.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\02\\31.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\01.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\02.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\03.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\04.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\05.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\06.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\07.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\08.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\09.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\10.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\11.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\12.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\13.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\14.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\15.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\16.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\17.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\18.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\19.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\20.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\21.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\22.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\23.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\24.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\25.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\26.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\27.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\28.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\29.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\30.json\n",
      "Extra_Storage\\ArchiveteamTest\\2016\\01\\31.json\n"
     ]
    }
   ],
   "source": [
    "extractTweets('ArchiveteamTest', '2016', '02')\n",
    "extractTweets('ArchiveteamTest', '2016', '01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
